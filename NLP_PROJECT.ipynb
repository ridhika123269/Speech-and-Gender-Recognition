{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee90548f",
   "metadata": {},
   "source": [
    "# SHOURYA GUPTA - 19BCE1704\n",
    "\n",
    "# RIDHIKA SAHNI - 19BCE1697\n",
    "\n",
    "\n",
    "# NLP PROJECT - COURT REPORTER\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f018728a",
   "metadata": {},
   "source": [
    "# Start recording - make an audio file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40da58c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python-sounddevice allows you to record audio from your microphone and store it as a NumPy array. \n",
    "#This is a handy datatype for sound processing that can be converted to WAV format for storage using the scipy.io.wavfile module.\n",
    "\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "fs = 44100  # Sample rate\n",
    "seconds = 10  # Duration of recording\n",
    "\n",
    "myrecording = sd.rec(int(seconds * fs), samplerate=fs, channels=2)\n",
    "sd.wait()  # Wait until recording is finished\n",
    "write('output.wav', fs, myrecording)  # Save as WAV file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd29966",
   "metadata": {},
   "source": [
    "# Create a file to store all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4ea6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#File created \n",
    "# a = append\n",
    "#Plus sign indicates both read and write for Python create file operation.\n",
    "\n",
    "f = open(\"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\document.txt\",\"a+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff098722",
   "metadata": {},
   "source": [
    "# Extract text from the audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff3f42db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: pydub in c:\\programdata\\anaconda3\\lib\\site-packages (0.25.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install SpeechRecognition pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5699c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Processing quotes and I am trying to make a speech recognizer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import soundfile\n",
    "\n",
    "data, samplerate = soundfile.read('output.wav')\n",
    "soundfile.write('new.wav', data, samplerate, subtype='PCM_16')\n",
    "\n",
    "filename = \"new.wav\"\n",
    "\n",
    "# initialize the recognizer\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# open the file\n",
    "with sr.AudioFile(filename) as source:\n",
    "    # listen for the data (load audio to memory)\n",
    "    audio_data = r.record(source)\n",
    "    # recognize (convert from speech to text)\n",
    "    \n",
    "    #API powered by Google’s AI technologies\n",
    "    text = r.recognize_google(audio_data)\n",
    "    print(text)\n",
    "    \n",
    "f.write(\"Text : \")\n",
    "f.write(text)\n",
    "f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5d0f7e",
   "metadata": {},
   "source": [
    "# Want to hear the text? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6b7c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfcc09ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "\n",
    "def SpeakText(command):\n",
    "    # Initialize the engine\n",
    "    engine = pyttsx3.init()\n",
    "    engine.say(command)\n",
    "    engine.runAndWait()\n",
    "    \n",
    "SpeakText(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fddce4",
   "metadata": {},
   "source": [
    "# Recognise the gender by voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e075d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning model that is able to detect and recognize gender just by your voice tone using Tensorflow framework in Python.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7053ae1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/cv-other-train/sample-069205.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/cv-valid-train/sample-063134.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/cv-other-train/sample-080873.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/cv-other-train/sample-105595.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/cv-valid-train/sample-144613.npy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                filename  gender\n",
       "0  data/cv-other-train/sample-069205.npy  female\n",
       "1  data/cv-valid-train/sample-063134.npy  female\n",
       "2  data/cv-other-train/sample-080873.npy  female\n",
       "3  data/cv-other-train/sample-105595.npy  female\n",
       "4  data/cv-valid-train/sample-144613.npy  female"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset used : https://www.kaggle.com/mozillaorg/common-voice\n",
    "# We extract these genre-labeled samples and perform gender recognition.\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\balanced-all.csv\")\n",
    "df.head()\n",
    "\n",
    "# It links each audio sample's file path to its appropriate gender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d39d4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66933</th>\n",
       "      <td>data/cv-valid-train/sample-171098.npy</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66934</th>\n",
       "      <td>data/cv-other-train/sample-022864.npy</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66935</th>\n",
       "      <td>data/cv-valid-train/sample-080933.npy</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66936</th>\n",
       "      <td>data/cv-other-train/sample-012026.npy</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66937</th>\n",
       "      <td>data/cv-other-train/sample-013841.npy</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    filename gender\n",
       "66933  data/cv-valid-train/sample-171098.npy   male\n",
       "66934  data/cv-other-train/sample-022864.npy   male\n",
       "66935  data/cv-valid-train/sample-080933.npy   male\n",
       "66936  data/cv-other-train/sample-012026.npy   male\n",
       "66937  data/cv-other-train/sample-013841.npy   male"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b84c869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 66938\n",
      "Total male samples: 33469\n",
      "Total female samples: 33469\n"
     ]
    }
   ],
   "source": [
    "#BALANCED DATASET\n",
    "\n",
    "# get total samples\n",
    "n_samples = len(df)\n",
    "# get total male samples\n",
    "n_male_samples = len(df[df['gender'] == 'male'])\n",
    "# get total female samples\n",
    "n_female_samples = len(df[df['gender'] == 'female'])\n",
    "print(\"Total samples:\", n_samples)\n",
    "print(\"Total male samples:\", n_male_samples)\n",
    "print(\"Total female samples:\", n_female_samples)\n",
    "\n",
    "# a large number of balanced audio samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2abf8a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {\n",
    "    \"male\": 1,\n",
    "    \"female\": 0\n",
    "}\n",
    "\n",
    "def load_data(vector_length=128):\n",
    "\n",
    "     # function to load gender recognition dataset from `data` folder - INITIALLY\n",
    "    if not os.path.isdir(\"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\results\"):\n",
    "        os.mkdir(\"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\results\")\n",
    "    \n",
    "    #  After the second run, this will load from results/features.npy and results/labels.npy files as it is much faster!\n",
    "    if os.path.isfile(\"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\results\\\\features.npy\") and os.path.isfile(\"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\results\\\\labels.npy\"):\n",
    "        X = np.load(\"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\results\\\\features.npy\")\n",
    "        y = np.load(\"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\results\\\\labels.npy\")\n",
    "        return X, y\n",
    "\n",
    "    df = pd.read_csv(\"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\balanced-all.csv\")\n",
    "\n",
    "    n_samples = len(df)\n",
    "\n",
    "    n_male_samples = len(df[df['gender'] == 'male'])\n",
    "    \n",
    "    n_female_samples = len(df[df['gender'] == 'female'])\n",
    "    print(\"Total samples:\", n_samples)\n",
    "    print(\"Total male samples:\", n_male_samples)\n",
    "    print(\"Total female samples:\", n_female_samples)\n",
    "    \n",
    "    # initialize an empty array for all audio features\n",
    "    X = np.zeros((n_samples, vector_length))\n",
    "    \n",
    "    # initialize an empty array for all audio labels (1 for male and 0 for female)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    for i, (filename, gender) in tqdm.tqdm(enumerate(zip(df['filename'], df['gender'])), \"Loading data\", total=n_samples):\n",
    "        features = np.load(filename)\n",
    "        X[i] = features\n",
    "        y[i] = label2int[gender]\n",
    "\n",
    "    # save the audio features and labels into files\n",
    "    # so we won't load each one of them next run\n",
    "    np.save(\"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\results\\\\features.npy\", X)\n",
    "    np.save(\"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\results\\\\labels.npy\", y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1c0b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Dataset: The sample of data used to fit the model.\n",
    "# Validation data. During training, validation data infuses new data into the model that it hasn’t evaluated before. Validation data provides the first test against unseen data, allowing data scientists to evaluate how well the model makes predictions based on the new data. Not all data scientists use validation data, but it can provide some helpful information to optimize hyperparameters, which influence how the model assesses data.\n",
    "\n",
    "# A hyperparameter is a parameter that is set before the learning process begins. These parameters are tunable and can directly affect how well a model trains. Some examples of hyperparameters in machine learning:\n",
    "#Learning Rate\n",
    "# Number of Epochs\n",
    "# Momentum\n",
    "# Regularization constant\n",
    "# Number of branches in a decision tree\n",
    "# Number of clusters in a clustering algorithm (like k-means)\n",
    "\n",
    "# Test Dataset: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "def split_data(X, y, test_size=0.1, valid_size=0.1):\n",
    "    # split training set and testing set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=7)\n",
    "    # split training set and validation set\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=valid_size, random_state=7)\n",
    "   \n",
    "    return {\n",
    "        \"X_train\": X_train,\n",
    "        \"X_valid\": X_valid,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_valid\": y_valid,\n",
    "        \"y_test\": y_test\n",
    "    }\n",
    "\n",
    "# We're using sklearn's train_test_split() convenient function, which will shuffle our dataset and splits it into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b531d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "X, y = load_data()\n",
    "# split the data into training, validation and testing sets\n",
    "data = split_data(X, y, test_size=0.1, valid_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "993c4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep feed-forward neural network with 5 hidden layers \n",
    "# Feed Forward neural - information is only processed in one direction\n",
    "\n",
    "\n",
    "def create_model(vector_length=128):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(vector_length,)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    # one output neuron with sigmoid activation function, 0 means female, 1 means male\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    # using binary crossentropy as it's male/female classification (binary)\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"], optimizer=\"adam\")\n",
    "    # print summary of the model\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# ReLU stands for rectified linear unit, and is a type of activation function. Mathematically, it is defined as y = max(0, x). \n",
    "# Sigmoid activation function, For small values (<-5), sigmoid returns a value close to zero, and for large values (>5) the result of the function gets close to 1.\n",
    "\n",
    "# in the output layer, the model will output the scalar 1 (or close to it) when the audio's speaker is a male, and female when it's closer to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e1c3577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 156,545\n",
      "Trainable params: 156,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25bbf99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "848/848 [==============================] - 10s 9ms/step - loss: 0.5689 - accuracy: 0.7667 - val_loss: 0.3719 - val_accuracy: 0.8451\n",
      "Epoch 2/100\n",
      "848/848 [==============================] - 6s 7ms/step - loss: 0.4155 - accuracy: 0.8345 - val_loss: 0.3401 - val_accuracy: 0.8699\n",
      "Epoch 3/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.3783 - accuracy: 0.8518 - val_loss: 0.3262 - val_accuracy: 0.8727\n",
      "Epoch 4/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.3619 - accuracy: 0.8615 - val_loss: 0.3008 - val_accuracy: 0.8825\n",
      "Epoch 5/100\n",
      "848/848 [==============================] - 6s 7ms/step - loss: 0.3435 - accuracy: 0.8666 - val_loss: 0.2956 - val_accuracy: 0.8845\n",
      "Epoch 6/100\n",
      "848/848 [==============================] - 8s 9ms/step - loss: 0.3363 - accuracy: 0.8698 - val_loss: 0.2829 - val_accuracy: 0.8908\n",
      "Epoch 7/100\n",
      "848/848 [==============================] - 6s 7ms/step - loss: 0.3310 - accuracy: 0.8729 - val_loss: 0.2824 - val_accuracy: 0.8903\n",
      "Epoch 8/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.3212 - accuracy: 0.8764 - val_loss: 0.2705 - val_accuracy: 0.8943\n",
      "Epoch 9/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.3193 - accuracy: 0.8799 - val_loss: 0.2904 - val_accuracy: 0.8888\n",
      "Epoch 10/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.3125 - accuracy: 0.8823 - val_loss: 0.2776 - val_accuracy: 0.8986\n",
      "Epoch 11/100\n",
      "848/848 [==============================] - 8s 9ms/step - loss: 0.3025 - accuracy: 0.8858 - val_loss: 0.2595 - val_accuracy: 0.9001\n",
      "Epoch 12/100\n",
      "848/848 [==============================] - 6s 7ms/step - loss: 0.2987 - accuracy: 0.8872 - val_loss: 0.2640 - val_accuracy: 0.8986\n",
      "Epoch 13/100\n",
      "848/848 [==============================] - 7s 9ms/step - loss: 0.2979 - accuracy: 0.8869 - val_loss: 0.2568 - val_accuracy: 0.9019\n",
      "Epoch 14/100\n",
      "848/848 [==============================] - 6s 7ms/step - loss: 0.2926 - accuracy: 0.8892 - val_loss: 0.2509 - val_accuracy: 0.9021\n",
      "Epoch 15/100\n",
      "848/848 [==============================] - 8s 9ms/step - loss: 0.2922 - accuracy: 0.8901 - val_loss: 0.2581 - val_accuracy: 0.9037\n",
      "Epoch 16/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2931 - accuracy: 0.8912 - val_loss: 0.2801 - val_accuracy: 0.8968\n",
      "Epoch 17/100\n",
      "848/848 [==============================] - 6s 8ms/step - loss: 0.2856 - accuracy: 0.8928 - val_loss: 0.2558 - val_accuracy: 0.9026\n",
      "Epoch 18/100\n",
      "848/848 [==============================] - 8s 9ms/step - loss: 0.2829 - accuracy: 0.8933 - val_loss: 0.2488 - val_accuracy: 0.9042\n",
      "Epoch 19/100\n",
      "848/848 [==============================] - 6s 7ms/step - loss: 0.2821 - accuracy: 0.8943 - val_loss: 0.2475 - val_accuracy: 0.9029\n",
      "Epoch 20/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2807 - accuracy: 0.8954 - val_loss: 0.2554 - val_accuracy: 0.9044\n",
      "Epoch 21/100\n",
      "848/848 [==============================] - 6s 7ms/step - loss: 0.2740 - accuracy: 0.8958 - val_loss: 0.2470 - val_accuracy: 0.9107\n",
      "Epoch 22/100\n",
      "848/848 [==============================] - 7s 9ms/step - loss: 0.2783 - accuracy: 0.8961 - val_loss: 0.2441 - val_accuracy: 0.9099\n",
      "Epoch 23/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2742 - accuracy: 0.8979 - val_loss: 0.2383 - val_accuracy: 0.9119\n",
      "Epoch 24/100\n",
      "848/848 [==============================] - 6s 7ms/step - loss: 0.2719 - accuracy: 0.8974 - val_loss: 0.2384 - val_accuracy: 0.9064\n",
      "Epoch 25/100\n",
      "848/848 [==============================] - 8s 9ms/step - loss: 0.2749 - accuracy: 0.8984 - val_loss: 0.2401 - val_accuracy: 0.9102\n",
      "Epoch 26/100\n",
      "848/848 [==============================] - 6s 7ms/step - loss: 0.2696 - accuracy: 0.8999 - val_loss: 0.2463 - val_accuracy: 0.9069\n",
      "Epoch 27/100\n",
      "848/848 [==============================] - 7s 9ms/step - loss: 0.2689 - accuracy: 0.8993 - val_loss: 0.2428 - val_accuracy: 0.9069\n",
      "Epoch 28/100\n",
      "848/848 [==============================] - 6s 7ms/step - loss: 0.2708 - accuracy: 0.8990 - val_loss: 0.2399 - val_accuracy: 0.9069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12f8ba01f70>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use tensorboard to view metrics\n",
    "tensorboard = TensorBoard(log_dir=\"logs\")\n",
    "\n",
    "# define early stopping to stop training after 5 epochs of not improving\n",
    "early_stopping = EarlyStopping(mode=\"min\", patience=5, restore_best_weights=True)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "# train the model using the training set and validating using validation set\n",
    "model.fit(data[\"X_train\"], data[\"y_train\"], epochs=epochs, batch_size=batch_size, validation_data=(data[\"X_valid\"], data[\"y_valid\"]),\n",
    "          callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8c770cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to a file\n",
    "model.save(\"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\results\\\\model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be15f6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model using 6694 samples...\n",
      "Loss: 0.2401\n",
      "Accuracy: 91.29%\n"
     ]
    }
   ],
   "source": [
    "# evaluating the model using the testing set\n",
    "print(f\"Evaluating the model using {len(data['X_test'])} samples...\")\n",
    "loss, accuracy = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d7da145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Extract features from that audio and feed it to the model to retrieve results\n",
    "\n",
    "def extract_feature(file_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract feature from audio file `file_name`\n",
    "        Features supported:\n",
    "            - MFCC (mfcc)\n",
    "            - Chroma (chroma)\n",
    "            - MEL Spectrogram Frequency (mel)\n",
    "            - Contrast (contrast)\n",
    "            - Tonnetz (tonnetz)\n",
    "        e.g:\n",
    "        `features = extract_feature(path, mel=True, mfcc=True)`\n",
    "    \"\"\"\n",
    "    mfcc = kwargs.get(\"mfcc\")\n",
    "    chroma = kwargs.get(\"chroma\")\n",
    "    mel = kwargs.get(\"mel\")\n",
    "    contrast = kwargs.get(\"contrast\")\n",
    "    tonnetz = kwargs.get(\"tonnetz\")\n",
    "    X, sample_rate = librosa.core.load(file_name)\n",
    "    if chroma or contrast:\n",
    "        stft = np.abs(librosa.stft(X))\n",
    "    result = np.array([])\n",
    "    if mfcc:\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result = np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, chroma))\n",
    "    if mel:\n",
    "        mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, mel))\n",
    "    if contrast:\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, contrast))\n",
    "    if tonnetz:\n",
    "        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, tonnetz))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54b67a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 156,545\n",
      "Trainable params: 156,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Result: female\n",
      "Probabilities::: Male: 6.51%    Female: 93.49%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# audio file path passed from the command line from test folder\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description=\"\"\"Gender recognition script, this will load the model you trained, \n",
    "                                    and perform inference on a sample you provide (either using your voice or a file)\"\"\")\n",
    "parser.add_argument(\"-f\", \"--file\", help=\"The path to the file, preferred to be in WAV format\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# But here, we pass our own audio file - for better evaluation\n",
    "\n",
    "file = \"C:\\\\Users\\\\Shourya\\\\output.wav\"\n",
    "\n",
    "#file = args.file\n",
    "\n",
    "\n",
    "\n",
    "# construct the model\n",
    "model = create_model()\n",
    "# load the saved/trained weights\n",
    "model.load_weights(\"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\results\\\\model.h5\")\n",
    "\n",
    "\n",
    "#if not file or not os.path.isfile(file):\n",
    "    # if file not provided, or it doesn't exist, use your voice\n",
    "    #print(\"Please talk\")\n",
    "    # put the file name here\n",
    "    #file = \"test.wav\"\n",
    "    # record the file (start talking)\n",
    "    #record_to_file(file)\n",
    "\n",
    "    \n",
    "# extract features and reshape it\n",
    "features = extract_feature(file, mel=True).reshape(1, -1)\n",
    "# predict the gender!\n",
    "male_prob = model.predict(features)[0][0]\n",
    "female_prob = 1 - male_prob\n",
    "gender = \"male\" if male_prob > female_prob else \"female\"\n",
    "# show the result!\n",
    "print(\"Result:\", gender)\n",
    "print(f\"Probabilities::: Male: {male_prob*100:.2f}%    Female: {female_prob*100:.2f}%\")\n",
    "\n",
    "f.write(\"Gender : \")\n",
    "f.write(gender)\n",
    "f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2377d2cf",
   "metadata": {},
   "source": [
    "# Different outputs to show gender recognition by voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bcfe2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 156,545\n",
      "Trainable params: 156,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Result: female\n",
      "Probabilities::: Male: 0.02%    Female: 99.98%\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description=\"\"\"Gender recognition script, this will load the model you trained, \n",
    "                                    and perform inference on a sample you provide (either using your voice or a file)\"\"\")\n",
    "parser.add_argument(\"-f\", \"--file\", help=\"The path to the file, preferred to be in WAV format\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "file = \"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\shourya1.wav\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# construct the model\n",
    "model = create_model()\n",
    "\n",
    "model.load_weights(\"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\results\\\\model.h5\")\n",
    "\n",
    "\n",
    "#if not file or not os.path.isfile(file):\n",
    "    # if file not provided, or it doesn't exist, use your voice\n",
    "    #print(\"Please talk\")\n",
    "    # put the file name here\n",
    "    #file = \"test.wav\"\n",
    "    # record the file (start talking)\n",
    "    #record_to_file(file)\n",
    "\n",
    "    \n",
    "\n",
    "features = extract_feature(file, mel=True).reshape(1, -1)\n",
    "\n",
    "\n",
    "male_prob = model.predict(features)[0][0]\n",
    "female_prob = 1 - male_prob\n",
    "gender = \"male\" if male_prob > female_prob else \"female\"\n",
    "\n",
    "print(\"Result:\", gender)\n",
    "print(f\"Probabilities::: Male: {male_prob*100:.2f}%    Female: {female_prob*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description=\"\"\"Gender recognition script, this will load the model you trained, \n",
    "                                    and perform inference on a sample you provide (either using your voice or a file)\"\"\")\n",
    "parser.add_argument(\"-f\", \"--file\", help=\"The path to the file, preferred to be in WAV format\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "file = \"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\ritik.wav\"\n",
    "\n",
    "#file = args.file\n",
    "\n",
    "\n",
    "\n",
    "# construct the model\n",
    "model = create_model()\n",
    "# load the saved/trained weights\n",
    "model.load_weights(\"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\results\\\\model.h5\")\n",
    "\n",
    "\n",
    "#if not file or not os.path.isfile(file):\n",
    "    # if file not provided, or it doesn't exist, use your voice\n",
    "    #print(\"Please talk\")\n",
    "    # put the file name here\n",
    "    #file = \"test.wav\"\n",
    "    # record the file (start talking)\n",
    "    #record_to_file(file)\n",
    "\n",
    "    \n",
    "# extract features and reshape it\n",
    "features = extract_feature(file, mel=True).reshape(1, -1)\n",
    "# predict the gender!\n",
    "male_prob = model.predict(features)[0][0]\n",
    "female_prob = 1 - male_prob\n",
    "gender = \"male\" if male_prob > female_prob else \"female\"\n",
    "# show the result!\n",
    "print(\"Result:\", gender)\n",
    "print(f\"Probabilities::: Male: {male_prob*100:.2f}%    Female: {female_prob*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d4323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description=\"\"\"Gender recognition script, this will load the model you trained, \n",
    "                                    and perform inference on a sample you provide (either using your voice or a file)\"\"\")\n",
    "parser.add_argument(\"-f\", \"--file\", help=\"The path to the file, preferred to be in WAV format\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "file = \"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\shourya_proj.wav\"\n",
    "\n",
    "#file = args.file\n",
    "\n",
    "\n",
    "\n",
    "# construct the model\n",
    "model = create_model()\n",
    "# load the saved/trained weights\n",
    "model.load_weights(\"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\results\\\\model.h5\")\n",
    "\n",
    "\n",
    "#if not file or not os.path.isfile(file):\n",
    "    # if file not provided, or it doesn't exist, use your voice\n",
    "    #print(\"Please talk\")\n",
    "    # put the file name here\n",
    "    #file = \"test.wav\"\n",
    "    # record the file (start talking)\n",
    "    #record_to_file(file)\n",
    "\n",
    "    \n",
    "# extract features and reshape it\n",
    "features = extract_feature(file, mel=True).reshape(1, -1)\n",
    "# predict the gender!\n",
    "male_prob = model.predict(features)[0][0]\n",
    "female_prob = 1 - male_prob\n",
    "gender = \"male\" if male_prob > female_prob else \"female\"\n",
    "# show the result!\n",
    "print(\"Result:\", gender)\n",
    "print(f\"Probabilities::: Male: {male_prob*100:.2f}%    Female: {female_prob*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c56cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description=\"\"\"Gender recognition script, this will load the model you trained, \n",
    "                                    and perform inference on a sample you provide (either using your voice or a file)\"\"\")\n",
    "parser.add_argument(\"-f\", \"--file\", help=\"The path to the file, preferred to be in WAV format\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "file = \"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\ridhika.wav\"\n",
    "\n",
    "#file = args.file\n",
    "\n",
    "\n",
    "\n",
    "# construct the model\n",
    "model = create_model()\n",
    "# load the saved/trained weights\n",
    "model.load_weights(\"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\results\\\\model.h5\")\n",
    "\n",
    "\n",
    "#if not file or not os.path.isfile(file):\n",
    "    # if file not provided, or it doesn't exist, use your voice\n",
    "    #print(\"Please talk\")\n",
    "    # put the file name here\n",
    "    #file = \"test.wav\"\n",
    "    # record the file (start talking)\n",
    "    #record_to_file(file)\n",
    "\n",
    "    \n",
    "# extract features and reshape it\n",
    "features = extract_feature(file, mel=True).reshape(1, -1)\n",
    "# predict the gender!\n",
    "male_prob = model.predict(features)[0][0]\n",
    "female_prob = 1 - male_prob\n",
    "gender = \"male\" if male_prob > female_prob else \"female\"\n",
    "# show the result!\n",
    "print(\"Result:\", gender)\n",
    "print(f\"Probabilities::: Male: {male_prob*100:.2f}%    Female: {female_prob*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "584446f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099178d7",
   "metadata": {},
   "source": [
    "# Print the generated document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70892928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text : Natural Language Processing project I am trying to make a speech recognizer\n",
      "Gender : female\n",
      "Text : Language Processing quotes and I am trying to make a speech recognizer\n",
      "Gender : female\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"C:\\\\Users\\\\Shourya\\\\Desktop\\\\gender-recognition-by-voice-master\\\\document.txt\", \"r+\") as file1:\n",
    "    # Reading form a file\n",
    "    print(file1.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b4201",
   "metadata": {},
   "source": [
    "# Thank you :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
